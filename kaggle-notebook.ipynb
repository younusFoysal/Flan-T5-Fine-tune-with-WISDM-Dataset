{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada18381",
   "metadata": {},
   "source": [
    "# FLAN-T5 Fine-tuning for WISDM Activity Recognition (Kaggle)\n",
    "\n",
    "This notebook fine-tunes FLAN-T5 on the WISDM accelerometer dataset by converting time-series sensor data into text format for sequence-to-text generation.\n",
    "\n",
    "**Before running:**\n",
    "1. Upload your WISDM dataset files to Kaggle\n",
    "2. Add the dataset to your notebook\n",
    "3. Update the dataset path to point to your uploaded files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995d2a2",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages - fix compatibility issues\n",
    "!pip install -q protobuf==3.20.3\n",
    "!pip install -q transformers datasets torch scikit-learn tqdm accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d3836",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108e0b8",
   "metadata": {},
   "source": [
    "## Step 3: Load and Process WISDM Dataset\n",
    "\n",
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 1: Load WISDM Data ====================\n",
    "def load_wisdm_data(file_path):\n",
    "    \"\"\"\n",
    "    Load WISDM raw accelerometer data from text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to WISDM_ar_v1.1_raw.txt\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [user, activity, timestamp, x, y, z]\n",
    "    \"\"\"\n",
    "    print(\"Loading WISDM data...\")\n",
    "\n",
    "    # Read the file line by line and parse\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Remove trailing semicolon and whitespace\n",
    "            line = line.strip().rstrip(';')\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parts = line.split(',')\n",
    "                if len(parts) == 6:\n",
    "                    user = int(parts[0])\n",
    "                    activity = parts[1]\n",
    "                    timestamp = int(parts[2])\n",
    "                    x = float(parts[3])\n",
    "                    y = float(parts[4])\n",
    "                    z = float(parts[5])\n",
    "                    data.append([user, activity, timestamp, x, y, z])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['user', 'activity', 'timestamp', 'x', 'y', 'z'])\n",
    "    print(f\"Loaded {len(df)} sensor readings\")\n",
    "    print(f\"Activities: {df['activity'].unique()}\")\n",
    "    print(f\"Activity distribution:\\n{df['activity'].value_counts()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a013f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 2: Create Sliding Windows ====================\n",
    "def create_sliding_windows(df, window_size=80, step_size=40):\n",
    "    \"\"\"\n",
    "    Create sliding windows from time-series data for each user and activity.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with sensor readings\n",
    "        window_size (int): Number of samples per window (default: 80 â‰ˆ 4 seconds at 20Hz)\n",
    "        step_size (int): Step size for sliding window (default: 40, 50% overlap)\n",
    "\n",
    "    Returns:\n",
    "        list: List of (window_data, activity_label) tuples\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating sliding windows (size={window_size}, step={step_size})...\")\n",
    "\n",
    "    windows = []\n",
    "\n",
    "    # Group by user and activity to maintain continuity\n",
    "    for (user, activity), group in tqdm(df.groupby(['user', 'activity'])):\n",
    "        # Sort by timestamp\n",
    "        group = group.sort_values('timestamp')\n",
    "\n",
    "        # Extract sensor values\n",
    "        sensor_data = group[['x', 'y', 'z']].values\n",
    "\n",
    "        # Create windows\n",
    "        for i in range(0, len(sensor_data) - window_size + 1, step_size):\n",
    "            window = sensor_data[i:i + window_size]\n",
    "            if len(window) == window_size:\n",
    "                windows.append((window, activity))\n",
    "\n",
    "    print(f\"Created {len(windows)} windows\")\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 3: Convert Windows to Text ====================\n",
    "def window_to_text(window):\n",
    "    \"\"\"\n",
    "    Convert a numeric sensor window to text format for T5 input.\n",
    "\n",
    "    Args:\n",
    "        window (np.ndarray): Array of shape (window_size, 3) with x, y, z values\n",
    "\n",
    "    Returns:\n",
    "        str: Text representation of the window\n",
    "    \"\"\"\n",
    "    # Convert sensor readings to text format\n",
    "    # Use statistics to make input more meaningful and avoid echo patterns\n",
    "    x_vals = window[:, 0]\n",
    "    y_vals = window[:, 1]\n",
    "    z_vals = window[:, 2]\n",
    "\n",
    "    # Calculate statistics to reduce dimensionality\n",
    "    x_mean, x_std = float(np.mean(x_vals)), float(np.std(x_vals))\n",
    "    y_mean, y_std = float(np.mean(y_vals)), float(np.std(y_vals))\n",
    "    z_mean, z_std = float(np.mean(z_vals)), float(np.std(z_vals))\n",
    "\n",
    "    # Create a descriptive text input using only statistics\n",
    "    # Avoid comma-separated numbers that model might echo\n",
    "    text = (f\"Accelerometer data: \"\n",
    "            f\"x-axis mean {x_mean:.2f} std {x_std:.2f}, \"\n",
    "            f\"y-axis mean {y_mean:.2f} std {y_std:.2f}, \"\n",
    "            f\"z-axis mean {z_mean:.2f} std {z_std:.2f}. \"\n",
    "            f\"What activity is this?\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_text_dataset(windows):\n",
    "    \"\"\"\n",
    "    Convert windows to text dataset format for T5.\n",
    "\n",
    "    Args:\n",
    "        windows (list): List of (window_data, activity_label) tuples\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'input_text' and 'target_text' columns\n",
    "    \"\"\"\n",
    "    print(\"\\nConverting windows to text format...\")\n",
    "\n",
    "    data = []\n",
    "    for window, activity in tqdm(windows):\n",
    "        input_text = window_to_text(window)\n",
    "        # Make target more explicit for T5\n",
    "        target_text = activity.strip()\n",
    "        data.append({'input_text': input_text, 'target_text': target_text})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Created {len(df)} text examples\")\n",
    "    print(f\"\\nSample input: {df['input_text'].iloc[0]}\")\n",
    "    print(f\"Sample target: {df['target_text'].iloc[0]}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db622f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 4: Prepare Dataset for T5 ====================\n",
    "def prepare_dataset(text_df, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Split data and create HuggingFace Dataset objects.\n",
    "\n",
    "    Args:\n",
    "        text_df (pd.DataFrame): DataFrame with input_text and target_text\n",
    "        test_size (float): Proportion for test set\n",
    "        val_size (float): Proportion for validation set (from training set)\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: Dictionary with train, validation, and test datasets\n",
    "    \"\"\"\n",
    "    print(\"\\nSplitting dataset...\")\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        text_df, test_size=test_size, random_state=42, stratify=text_df['target_text']\n",
    "    )\n",
    "\n",
    "    # Second split: train vs val\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=val_size, random_state=42, stratify=train_val_df['target_text']\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # Create HuggingFace datasets\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "        'validation': Dataset.from_pandas(val_df, preserve_index=False),\n",
    "        'test': Dataset.from_pandas(test_df, preserve_index=False)\n",
    "    })\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 5: Tokenization ====================\n",
    "def preprocess_function(examples, tokenizer, max_input_length=512, max_target_length=16):\n",
    "    \"\"\"\n",
    "    Tokenize input and target texts for T5.\n",
    "    T5 expects: \"task_name: input_text\"\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of examples from dataset\n",
    "        tokenizer: T5 tokenizer\n",
    "        max_input_length (int): Max length for input tokens\n",
    "        max_target_length (int): Max length for target tokens\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized inputs and labels\n",
    "    \"\"\"\n",
    "    # Add task prefix for T5\n",
    "    inputs = [\"classify activity: \" + text for text in examples['input_text']]\n",
    "    targets = examples['target_text']\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Replace padding token id with -100 so it's ignored in loss calculation\n",
    "    labels_array = np.array(labels['input_ids'])\n",
    "    labels_array[labels_array == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = labels_array.tolist()\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset_dict, tokenizer):\n",
    "    \"\"\"\n",
    "    Apply tokenization to all splits in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (DatasetDict): Dataset with train/val/test splits\n",
    "        tokenizer: T5 tokenizer\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: Tokenized dataset\n",
    "    \"\"\"\n",
    "    print(\"\\nTokenizing dataset...\")\n",
    "\n",
    "    tokenized_datasets = dataset_dict.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset_dict['train'].column_names\n",
    "    )\n",
    "\n",
    "    print(\"Tokenization complete!\")\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f52e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 6: Fine-tune FLAN-T5 ====================\n",
    "def train_model(tokenized_datasets, model, tokenizer, output_dir='./results', num_epochs=5):\n",
    "    \"\"\"\n",
    "    Fine-tune FLAN-T5 model using HuggingFace Trainer.\n",
    "\n",
    "    Args:\n",
    "        tokenized_datasets (DatasetDict): Tokenized train/val/test data\n",
    "        model: FLAN-T5 model\n",
    "        tokenizer: T5 tokenizer\n",
    "        output_dir (str): Directory to save model checkpoints\n",
    "        num_epochs (int): Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trained model trainer\n",
    "    \"\"\"\n",
    "    print(\"\\nSetting up training...\")\n",
    "\n",
    "    # Data collator for dynamic padding\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "        pad_to_multiple_of=8  # For better GPU utilization\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        \"\"\"Compute accuracy metrics during evaluation.\"\"\"\n",
    "        predictions, labels = eval_preds\n",
    "\n",
    "        # predictions are already token IDs when predict_with_generate=True\n",
    "        # If they're logits (3D array), we need to take argmax\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Ensure predictions are valid integers within vocabulary range\n",
    "        # Convert to int32 to prevent overflow\n",
    "        predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1).astype(np.int32)\n",
    "\n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100 in labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        # Also convert labels to int32\n",
    "        labels = np.clip(labels, 0, tokenizer.vocab_size - 1).astype(np.int32)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate exact match accuracy\n",
    "        accuracy = sum([pred.strip().lower() == label.strip().lower()\n",
    "                       for pred, label in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n",
    "\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    # Training arguments - optimized for classification\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,  # Standard learning rate for fine-tuning\n",
    "        per_device_train_batch_size=8,  # Reduced batch size for stability\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        report_to=\"none\",  # Disable wandb/tensorboard\n",
    "        seed=42,  # For reproducibility\n",
    "        gradient_accumulation_steps=2,  # Accumulate gradients for stability\n",
    "        optim=\"adafactor\"  # Use adafactor optimizer for better stability\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 7: Evaluate Model ====================\n",
    "def evaluate_model(trainer, tokenized_datasets, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Evaluate the fine-tuned model on test set.\n",
    "\n",
    "    Args:\n",
    "        trainer (Seq2SeqTrainer): Trained model trainer\n",
    "        tokenized_datasets (DatasetDict): Dataset with test split\n",
    "        tokenizer: T5 tokenizer\n",
    "        model: Fine-tuned model\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "    print(f\"Test Loss: {metrics['eval_loss']:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def test_predictions(model, tokenizer, test_inputs, num_samples=5):\n",
    "    \"\"\"\n",
    "    Generate predictions for sample inputs.\n",
    "\n",
    "    Args:\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: T5 tokenizer\n",
    "        test_inputs (list): List of test input texts\n",
    "        num_samples (int): Number of samples to test\n",
    "\n",
    "    Returns:\n",
    "        list: Generated predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing predictions on sample inputs...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    predictions = []\n",
    "    for i, input_text in enumerate(test_inputs[:num_samples]):\n",
    "        # Add task prefix to match training format\n",
    "        prefixed_input = \"classify activity: \" + input_text\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prefixed_input,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate prediction with constrained decoding\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=16,\n",
    "                num_beams=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "        # Decode prediction\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Input: {input_text[:150]}...\")\n",
    "        print(f\"Predicted Activity: {prediction}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72a90e",
   "metadata": {},
   "source": [
    "## Step 4: Run the Main Pipeline\n",
    "\n",
    "### Configuration\n",
    "Update the `WISDM_FILE` path based on where you added the WISDM dataset in Kaggle.\n",
    "For example, if you added it as input data, the path would be `/kaggle/input/<dataset-name>/WISDM_ar_v1.1_raw.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0492534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MAIN PIPELINE ====================\n",
    "print(\"=\"*60)\n",
    "print(\"FLAN-T5 Fine-tuning for WISDM Activity Recognition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "# UPDATE THIS PATH BASED ON YOUR KAGGLE DATASET INPUT\n",
    "# If you added dataset as input, use: /kaggle/input/<dataset-name>/WISDM_ar_v1.1_raw.txt\n",
    "#WISDM_FILE = \"/kaggle/input/wisdm-dataset/WISDM_ar_v1.1_raw.txt\"\n",
    "WISDM_FILE = \"/kaggle/input/har-dataset/WISDM_ar_v1.1_raw.txt\"\n",
    "MODEL_NAME = \"google/flan-t5-small\"  # Using small version for faster training\n",
    "OUTPUT_DIR = \"/kaggle/working/flan-t5-wisdm\"\n",
    "WINDOW_SIZE = 80  # ~4 seconds at 20Hz\n",
    "STEP_SIZE = 40    # 50% overlap\n",
    "NUM_EPOCHS = 5    # Increased epochs for better training\n",
    "\n",
    "# Check if WISDM data exists\n",
    "if not os.path.exists(WISDM_FILE):\n",
    "    print(f\"Error: WISDM data file not found at {WISDM_FILE}\")\n",
    "    print(\"Please ensure the WISDM dataset is added as input data to your Kaggle notebook.\")\n",
    "    print(f\"Current file path: {WISDM_FILE}\")\n",
    "else:\n",
    "    # Step 1: Load WISDM data\n",
    "    df = load_wisdm_data(WISDM_FILE)\n",
    "\n",
    "    # Optionally limit data for faster training (remove for full dataset)\n",
    "    # df = df.groupby('activity').head(10000)\n",
    "\n",
    "    # Step 2: Create sliding windows\n",
    "    windows = create_sliding_windows(df, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
    "\n",
    "    # Step 3: Convert to text dataset\n",
    "    text_df = create_text_dataset(windows)\n",
    "\n",
    "    # Optionally limit dataset size for demonstration (remove for full training)\n",
    "    # text_df = text_df.groupby('target_text').head(500)\n",
    "\n",
    "    # Step 4: Prepare train/val/test splits\n",
    "    dataset_dict = prepare_dataset(text_df)\n",
    "\n",
    "    # Step 5: Load model and tokenizer\n",
    "    print(f\"\\nLoading FLAN-T5 model: {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Step 6: Tokenize dataset\n",
    "    tokenized_datasets = tokenize_dataset(dataset_dict, tokenizer)\n",
    "\n",
    "    # Step 7: Train model\n",
    "    trainer = train_model(\n",
    "        tokenized_datasets,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    # Step 8: Evaluate model\n",
    "    metrics = evaluate_model(trainer, tokenized_datasets, tokenizer, model)\n",
    "\n",
    "    # Step 9: Test predictions on samples\n",
    "    test_inputs = [dataset_dict['test'][i]['input_text'] for i in range(min(5, len(dataset_dict['test'])))]\n",
    "    test_targets = [dataset_dict['test'][i]['target_text'] for i in range(min(5, len(dataset_dict['test'])))]\n",
    "\n",
    "    predictions = test_predictions(model, tokenizer, test_inputs)\n",
    "\n",
    "    # Print accuracy on test samples\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample Predictions vs Ground Truth:\")\n",
    "    print(\"=\"*60)\n",
    "    correct = 0\n",
    "    for i, (pred, target) in enumerate(zip(predictions, test_targets)):\n",
    "        match = pred.strip().lower() == target.strip().lower()\n",
    "        if match:\n",
    "            correct += 1\n",
    "        print(f\"Sample {i+1}: Predicted='{pred.strip()}' | Actual='{target.strip()}' | Match={match}\")\n",
    "\n",
    "    accuracy = (correct / len(predictions)) * 100 if predictions else 0\n",
    "    print(f\"\\nSample Accuracy: {accuracy:.1f}%\")\n",
    "\n",
    "    # Save final model\n",
    "    print(f\"\\nSaving final model to {OUTPUT_DIR}/final_model...\")\n",
    "    trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "    print(\"You can now use this model for activity recognition from accelerometer data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
